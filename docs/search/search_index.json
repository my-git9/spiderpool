{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spiderpool Introduction The Spiderpool is an IP Address Management (IPAM) CNI plugin that assigns IP addresses for kubernetes clusters. Currently, it is under developing stage, not ready for production environment yet. Any Container Network Interface (CNI) plugin supporting third-party IPAM plugins can use the Spiderpool, such as MacVLAN CNI , VLAN CNI , IPVLAN CNI etc. The Spiderpool also supports Multus CNI case to assign IP for multiple interfaces. More CNIs will be tested for integration with Spiderpool. Why Spiderpool Most overlay CNIs, like Cilium and Calico , have a good implementation of IPAM, so the Spiderpool is not intentionally designed for these cases, but may be integrated with them. The Spiderpool is specifically designed to use with underlay network, where administrators can accurately manage each IP. Currently, the community already has some IPAM plugins such as whereabout , kube-ipam , static , dhcp , and host-local , but few of them could help solve complex underlay-network issues, so we decide to develop the Spiderpool. BTW, there are also some CNI plugins that could work in the underlay mode, such as kube-ovn and coil . But the Spiderpool provides lots of different features. See Features for details. Features The Spiderpool provides a large number of different features as follows. Based on CRD storage, all operation could be done with kubernetes API-server. Support for assigning IP addresses with three options: IPv4-only, IPv6-only, and dual-stack. Support for working on the clusters with three options: IPv4-only, IPv6-only, and dual-stack. Support for creating multiple ippools. Different namespaces and applications could monopolize or share an ippool. An application could specify multiple backup ippool resources in case IP addresses in an ippool are out of use. Therefore, you neither need to scale up the IP resources in a fixed ippool, nor need to modify the application yaml to change an ippool. Support to bind a range of IP addresses to a single application. No need to hard code an IP list in a deployment yaml, which is not easy to modify. With Spiderpool, you only need to set the selector field of ippool and scale up or down the IP resource of an ippool dynamically. Support for always assigning the same IP address to a StatefulSet pod. Different pods in a single controller could get IP addresses from different subnets for an application deployed in different subnets or zones. Administrator could safely edit ippool resources, where the Spiderpool will help validate the modification and prevent from data race. Collect resources in real time, especially for solving IP leakage or slow collection, which may make new pod fail to assign IP addresses. Support ranges of CNI plugin that supports third-party IPAM plugins. Especially, the Spiderpool could help much for CNI like spiderflat , macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI . Especially support for Multus CNI case to assign IP for multiple interfaces. Have a good performance for assigning and collecting IP. Support to reserve IP that will not be assigned to any pod. Included metrics for looking into IP usage and issues. By CIDR Manager, it could automatically scale up and down the IP address of the ippool, to distribute IP resource more reasonable between ippool. Support for both ARM64 and ARM64. Components Refer to architecture for components. Installation Refer to installation . Quick Start Refer to demo . Development Development guide is a reference point for development helper commands. License Spiderpool is licensed under the Apache License, Version 2.0.","title":"Spiderpool"},{"location":"#spiderpool","text":"","title":"Spiderpool"},{"location":"#introduction","text":"The Spiderpool is an IP Address Management (IPAM) CNI plugin that assigns IP addresses for kubernetes clusters. Currently, it is under developing stage, not ready for production environment yet. Any Container Network Interface (CNI) plugin supporting third-party IPAM plugins can use the Spiderpool, such as MacVLAN CNI , VLAN CNI , IPVLAN CNI etc. The Spiderpool also supports Multus CNI case to assign IP for multiple interfaces. More CNIs will be tested for integration with Spiderpool.","title":"Introduction"},{"location":"#why-spiderpool","text":"Most overlay CNIs, like Cilium and Calico , have a good implementation of IPAM, so the Spiderpool is not intentionally designed for these cases, but may be integrated with them. The Spiderpool is specifically designed to use with underlay network, where administrators can accurately manage each IP. Currently, the community already has some IPAM plugins such as whereabout , kube-ipam , static , dhcp , and host-local , but few of them could help solve complex underlay-network issues, so we decide to develop the Spiderpool. BTW, there are also some CNI plugins that could work in the underlay mode, such as kube-ovn and coil . But the Spiderpool provides lots of different features. See Features for details.","title":"Why Spiderpool"},{"location":"#features","text":"The Spiderpool provides a large number of different features as follows. Based on CRD storage, all operation could be done with kubernetes API-server. Support for assigning IP addresses with three options: IPv4-only, IPv6-only, and dual-stack. Support for working on the clusters with three options: IPv4-only, IPv6-only, and dual-stack. Support for creating multiple ippools. Different namespaces and applications could monopolize or share an ippool. An application could specify multiple backup ippool resources in case IP addresses in an ippool are out of use. Therefore, you neither need to scale up the IP resources in a fixed ippool, nor need to modify the application yaml to change an ippool. Support to bind a range of IP addresses to a single application. No need to hard code an IP list in a deployment yaml, which is not easy to modify. With Spiderpool, you only need to set the selector field of ippool and scale up or down the IP resource of an ippool dynamically. Support for always assigning the same IP address to a StatefulSet pod. Different pods in a single controller could get IP addresses from different subnets for an application deployed in different subnets or zones. Administrator could safely edit ippool resources, where the Spiderpool will help validate the modification and prevent from data race. Collect resources in real time, especially for solving IP leakage or slow collection, which may make new pod fail to assign IP addresses. Support ranges of CNI plugin that supports third-party IPAM plugins. Especially, the Spiderpool could help much for CNI like spiderflat , macvlan CNI , vlan CNI , ipvlan CNI , sriov CNI , ovs CNI . Especially support for Multus CNI case to assign IP for multiple interfaces. Have a good performance for assigning and collecting IP. Support to reserve IP that will not be assigned to any pod. Included metrics for looking into IP usage and issues. By CIDR Manager, it could automatically scale up and down the IP address of the ippool, to distribute IP resource more reasonable between ippool. Support for both ARM64 and ARM64.","title":"Features"},{"location":"#components","text":"Refer to architecture for components.","title":"Components"},{"location":"#installation","text":"Refer to installation .","title":"Installation"},{"location":"#quick-start","text":"Refer to demo .","title":"Quick Start"},{"location":"#development","text":"Development guide is a reference point for development helper commands.","title":"Development"},{"location":"#license","text":"Spiderpool is licensed under the Apache License, Version 2.0.","title":"License"},{"location":"cmdref/spiderpool-agent/","text":"spiderpool-agent This page describes CLI options and ENV of spiderpool-agent. spiderpool-agent daemon Run the spiderpool agent daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710) spiderpool-agent shutdown Notify of stopping the spiderpool-agent daemon. spiderpool-agent metric Get local metrics. Options --port string http server port of local metric (default to 5711)","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent","text":"This page describes CLI options and ENV of spiderpool-agent.","title":"spiderpool-agent"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-daemon","text":"Run the spiderpool agent daemon.","title":"spiderpool-agent daemon"},{"location":"cmdref/spiderpool-agent/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map) --ipam-config-dir string config file for ipam plugin","title":"Options"},{"location":"cmdref/spiderpool-agent/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5711) SPIDERPOOL_HEALTH_PORT http port (default to 5710)","title":"ENV"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-shutdown","text":"Notify of stopping the spiderpool-agent daemon.","title":"spiderpool-agent shutdown"},{"location":"cmdref/spiderpool-agent/#spiderpool-agent-metric","text":"Get local metrics.","title":"spiderpool-agent metric"},{"location":"cmdref/spiderpool-agent/#options_1","text":"--port string http server port of local metric (default to 5711)","title":"Options"},{"location":"cmdref/spiderpool-controller/","text":"spiderpool-controller This page describes CLI options and ENV of spiderpool-controller. spiderpool-controller daemon Run the spiderpool controller daemon. Options --config-dir string config file path (default /tmp/spiderpool/config-map) ENV SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED enable GC ip of evicted pod (true|false, default to true) SPIDERPOOL_GC_EVICTED_POD_IP_DELAY delay to GC ip of evicted pod (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710) spiderpool-controller shutdown Notify of stopping spiderpool-controller daemon. spiderpool-controller metric Get local metrics. Options --port string http server port of local metric (default to 5721) spiderpool-controller status Show status: Whether local is controller leader ... Options --port string http server port of local metric (default to 5720)","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller","text":"This page describes CLI options and ENV of spiderpool-controller.","title":"spiderpool-controller"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-daemon","text":"Run the spiderpool controller daemon.","title":"spiderpool-controller daemon"},{"location":"cmdref/spiderpool-controller/#options","text":"--config-dir string config file path (default /tmp/spiderpool/config-map)","title":"Options"},{"location":"cmdref/spiderpool-controller/#env","text":"SPIDERPOOL_LOG_LEVEL log level (DEBUG|INFO|ERROR) SPIDERPOOL_ENABLED_METRIC enable metrics (true|false) SPIDERPOOL_METRIC_HTTP_PORT metric port (default to 5721) SPIDERPOOL_GC_IPPOOL_ENABLED enable GC ip in ippool, prior to other GC environment (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_ENABLED enable GC ip of terminating pod whose graceful-time times out (true|false, default to true) SPIDERPOOL_GC_TERMINATING_POD_IP_DELAY delay to GC ip after graceful-time times out (second, default to 0) SPIDERPOOL_GC_EVICTED_POD_IP_ENABLED enable GC ip of evicted pod (true|false, default to true) SPIDERPOOL_GC_EVICTED_POD_IP_DELAY delay to GC ip of evicted pod (second, default to 0) SPIDERPOOL_HEALTH_PORT http port (default to 5710)","title":"ENV"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-shutdown","text":"Notify of stopping spiderpool-controller daemon.","title":"spiderpool-controller shutdown"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-metric","text":"Get local metrics.","title":"spiderpool-controller metric"},{"location":"cmdref/spiderpool-controller/#options_1","text":"--port string http server port of local metric (default to 5721)","title":"Options"},{"location":"cmdref/spiderpool-controller/#spiderpool-controller-status","text":"Show status: Whether local is controller leader ...","title":"spiderpool-controller status"},{"location":"cmdref/spiderpool-controller/#options_2","text":"--port string http server port of local metric (default to 5720)","title":"Options"},{"location":"cmdref/spiderpoolctl/","text":"spiderpoolctl This page describes CLI usage of spiderpoolctl for debug. spiderpoolctl gc Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address) spiderpoolctl ip show Show a pod that is taking this IP. Options --ip string [required] ip spiderpoolctl ip release Try to release an IP. Options --ip string [optional] ip --force [optional] force release ip spiderpoolctl ip set Set IP to be taken by a pod. This will update ippool and workload endpoint resource. Options --ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl","text":"This page describes CLI usage of spiderpoolctl for debug.","title":"spiderpoolctl"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-gc","text":"Trigger the GC request to spiderpool-controller. --address string [optional] address for spider-controller (default to service address)","title":"spiderpoolctl gc"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-show","text":"Show a pod that is taking this IP.","title":"spiderpoolctl ip show"},{"location":"cmdref/spiderpoolctl/#options","text":"--ip string [required] ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-release","text":"Try to release an IP.","title":"spiderpoolctl ip release"},{"location":"cmdref/spiderpoolctl/#options_1","text":"--ip string [optional] ip --force [optional] force release ip","title":"Options"},{"location":"cmdref/spiderpoolctl/#spiderpoolctl-ip-set","text":"Set IP to be taken by a pod. This will update ippool and workload endpoint resource.","title":"spiderpoolctl ip set"},{"location":"cmdref/spiderpoolctl/#options_2","text":"--ip string [required] ip --pod string [required] pod name --namespace string [required] pod namespace --containerid string [required] pod container id --node string [required] the node name who the pod locates --interface string [required] pod interface who taking effect the ip","title":"Options"},{"location":"concepts/allocation/","text":"IP Allocation When a pod is creating, it will follow steps below to get an IP. Get all ippool candidates. For which ippool is used by a pod, the following rules are listed from high to low priority. Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used to specify an ippool. See Pod Annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used to specify an ippool. See namespace annotation for detail. Cluster default ippool. It can be set to \"clusterDefaultIPv4IPPool\" and \"clusterDefaultIPv6IPPool\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Filter valid ippool candidates. After getting IPv4 and IPv6 ippool candidates, it looks into each ippool and figures out whether it meets following rules, and learns which candidate ippool is available. The \"disable\" field of the ippool is \"false\" The \"ipversion\" field of the ippool must meet the claim The \"namespaceSelector\" field of the ippool must meet the namespace of the pod The \"podSelector\" field of the ippool must meet the pod The \"nodeSelector\" field of the ippool must meet the scheduled node of the pod The available IP resource of the ippool is not exhausted Assign IP from valid ippool candidates. When trying to assign IP from the ippool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the ippool and all ReservedIP instances When the pod controller is a StatefulSet, the pod will get an IP in sequence","title":"IP Allocation"},{"location":"concepts/allocation/#ip-allocation","text":"When a pod is creating, it will follow steps below to get an IP. Get all ippool candidates. For which ippool is used by a pod, the following rules are listed from high to low priority. Honor pod annotation. \"ipam.spidernet.io/ippool\" and \"ipam.spidernet.io/ippools\" could be used to specify an ippool. See Pod Annotation for detail. Namespace annotation. \"ipam.spidernet.io/defaultv4ippool\" and \"ipam.spidernet.io/defaultv6ippool\" could be used to specify an ippool. See namespace annotation for detail. Cluster default ippool. It can be set to \"clusterDefaultIPv4IPPool\" and \"clusterDefaultIPv6IPPool\" in the \"spiderpool-conf\" ConfigMap. See configuration for detail. Filter valid ippool candidates. After getting IPv4 and IPv6 ippool candidates, it looks into each ippool and figures out whether it meets following rules, and learns which candidate ippool is available. The \"disable\" field of the ippool is \"false\" The \"ipversion\" field of the ippool must meet the claim The \"namespaceSelector\" field of the ippool must meet the namespace of the pod The \"podSelector\" field of the ippool must meet the pod The \"nodeSelector\" field of the ippool must meet the scheduled node of the pod The available IP resource of the ippool is not exhausted Assign IP from valid ippool candidates. When trying to assign IP from the ippool candidates, it follows rules as below. The IP is not reserved by the \"exclude_ips\" field of the ippool and all ReservedIP instances When the pod controller is a StatefulSet, the pod will get an IP in sequence","title":"IP Allocation"},{"location":"concepts/annotation/","text":"Annotations Spiderpool provides annotations for configuring custom ippools and routes. Pod annotations For a pod, you can specify Spiderpool annotations for a special request. ipam.spidernet.io/ippool Specify the ippools used to allocate IP. ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\", \"v6-ippool2\"] } interface (string, optional): When integrated with multus CNI , it could specify which ippool is used to the interface. The interface information in the CNI request will be used as the default value when this field is not specified. ipv4pools (array, optional): Specify which ippool is used to allocate the IPv4 IP. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6pools (array, optional): Specify which ippool is used to allocate the IPv6 IP. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. ipam.spidernet.io/ippools It is similar to ipam.spidernet.io/ippool , but could be used to multiple interfaces case. BTW, ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\"], \"defaultRoute\": true },{ \"interface\": \"eth1\", \"ipv4pools\": [\"v4-ippool2\"], \"ipv6pools\": [\"v6-ippool2\"], \"defaultRoute\": false }] interface (string, required): Since the CNI request only carries the information of one interface, in the case of multiple interfaces, the interface field must be specified to distinguish. ipv4pools (array, optional): Specify which ippool is used to allocate IPv4 IP. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6pools (array, optional): Specify which ippool is used to allocate IPv6 IP. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. defaultRoute (bool, optional): If set to true, the IPAM plugin will return the default gateway route recorded in the ippool. For different interfaces, it is not recommended to use ippools of the same subnet. ipam.spidernet.io/routes You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes: |- [{ \"interface\": \"eth0\", \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" }] interface (string, required): The name of the interface over which the destination is reachable. dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address. ipam.spidernet.io/assigned-{INTERFACE} It is the IP allocation result of the interface. It is only used by Spiderpool, not reserved for users. ipam.spidernet.io/assigned-eth0: |- { \"interface\": \"eth0\", \"ipv4pool\": \"v4-ippool1\", \"ipv6pool\": \"v6-ippool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 } Namespace annotations Namespace could set following annotations to specify default ippools. They are valid for all Pods under the Namespace. ipam.spidernet.io/defaultv4ippool ipam.spidernet.io/defaultv4ippool: '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' If multiple ippools are listed, it will try to allocate IP from the later ippool when the former one is not allocatable. ipam.spidernet.io/defaultv6ippool ipam.spidernet.io/defaultv6ippool: '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' For other procedure, similar to Pod Annotations described above.","title":"Annotations"},{"location":"concepts/annotation/#annotations","text":"Spiderpool provides annotations for configuring custom ippools and routes.","title":"Annotations"},{"location":"concepts/annotation/#pod-annotations","text":"For a pod, you can specify Spiderpool annotations for a special request.","title":"Pod annotations"},{"location":"concepts/annotation/#ipamspidernetioippool","text":"Specify the ippools used to allocate IP. ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\", \"v6-ippool2\"] } interface (string, optional): When integrated with multus CNI , it could specify which ippool is used to the interface. The interface information in the CNI request will be used as the default value when this field is not specified. ipv4pools (array, optional): Specify which ippool is used to allocate the IPv4 IP. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6pools (array, optional): Specify which ippool is used to allocate the IPv6 IP. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required.","title":"ipam.spidernet.io/ippool"},{"location":"concepts/annotation/#ipamspidernetioippools","text":"It is similar to ipam.spidernet.io/ippool , but could be used to multiple interfaces case. BTW, ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\"], \"defaultRoute\": true },{ \"interface\": \"eth1\", \"ipv4pools\": [\"v4-ippool2\"], \"ipv6pools\": [\"v6-ippool2\"], \"defaultRoute\": false }] interface (string, required): Since the CNI request only carries the information of one interface, in the case of multiple interfaces, the interface field must be specified to distinguish. ipv4pools (array, optional): Specify which ippool is used to allocate IPv4 IP. When enableIPv4 in the spiderpool-conf ConfigMap is set to true, this field is required. ipv6pools (array, optional): Specify which ippool is used to allocate IPv6 IP. When enableIPv6 in the spiderpool-conf ConfigMap is set to true, this field is required. defaultRoute (bool, optional): If set to true, the IPAM plugin will return the default gateway route recorded in the ippool. For different interfaces, it is not recommended to use ippools of the same subnet.","title":"ipam.spidernet.io/ippools"},{"location":"concepts/annotation/#ipamspidernetioroutes","text":"You can use the following code to enable additional routes take effect. ipam.spidernet.io/routes: |- [{ \"interface\": \"eth0\", \"dst\": \"10.0.0.0/16\", \"gw\": \"192.168.1.1\" }] interface (string, required): The name of the interface over which the destination is reachable. dst (string, required): Network destination of the route. gw (string, required): The forwarding or next hop IP address.","title":"ipam.spidernet.io/routes"},{"location":"concepts/annotation/#ipamspidernetioassigned-interface","text":"It is the IP allocation result of the interface. It is only used by Spiderpool, not reserved for users. ipam.spidernet.io/assigned-eth0: |- { \"interface\": \"eth0\", \"ipv4pool\": \"v4-ippool1\", \"ipv6pool\": \"v6-ippool1\", \"ipv4\": \"172.16.0.100/16\", \"ipv6\": \"fd00::100/64\", \"vlan\": 100 }","title":"ipam.spidernet.io/assigned-{INTERFACE}"},{"location":"concepts/annotation/#namespace-annotations","text":"Namespace could set following annotations to specify default ippools. They are valid for all Pods under the Namespace.","title":"Namespace annotations"},{"location":"concepts/annotation/#ipamspidernetiodefaultv4ippool","text":"ipam.spidernet.io/defaultv4ippool: '[\"ns-v4-ippool1\",\"ns-v4-ippool2\"]' If multiple ippools are listed, it will try to allocate IP from the later ippool when the former one is not allocatable.","title":"ipam.spidernet.io/defaultv4ippool"},{"location":"concepts/annotation/#ipamspidernetiodefaultv6ippool","text":"ipam.spidernet.io/defaultv6ippool: '[\"ns-v6-ippool1\",\"ns-v6-ippool2\"]' For other procedure, similar to Pod Annotations described above.","title":"ipam.spidernet.io/defaultv6ippool"},{"location":"concepts/arch/","text":"Architecture Spiderpool consists of following components: Spiderpool IPAM plugin, a binary installed on each host. It is called by a CNI plugin to assign and release IP for a pod spiderpool-agent, deployed as a daemonset. It receives IPAM requests from the IPAM plugin, assigns and releases IP from the ippool resource spiderpool-controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. Refer to Resource Reclaim for details. It uses a webhook to watch the ippool resource, help the administrator to validate creation, modification, and deletion. spiderpoolctl, a CLI tool for debugging CRDs Spiderpool supports for the following CRDs: ippool CRD. It is used to store the IP resource for a subnet. Refer to ippool for detail. workloadendpoint CRD. It is used to store the IP assigned to a pod. Refer to workloadendpoint for detail. reservedip CRD. It is used to set the reserved IP, which will not be assigned to a pod even if you have set it in the ippool. Refer to reservedip for detail.","title":"Architecture"},{"location":"concepts/arch/#architecture","text":"Spiderpool consists of following components: Spiderpool IPAM plugin, a binary installed on each host. It is called by a CNI plugin to assign and release IP for a pod spiderpool-agent, deployed as a daemonset. It receives IPAM requests from the IPAM plugin, assigns and releases IP from the ippool resource spiderpool-controller, deployed as a deployment. It takes charge of reclaiming IP resource in ippool, to prevent IP from leaking after the pod does not take it. Refer to Resource Reclaim for details. It uses a webhook to watch the ippool resource, help the administrator to validate creation, modification, and deletion. spiderpoolctl, a CLI tool for debugging","title":"Architecture"},{"location":"concepts/arch/#crds","text":"Spiderpool supports for the following CRDs: ippool CRD. It is used to store the IP resource for a subnet. Refer to ippool for detail. workloadendpoint CRD. It is used to store the IP assigned to a pod. Refer to workloadendpoint for detail. reservedip CRD. It is used to set the reserved IP, which will not be assigned to a pod even if you have set it in the ippool. Refer to reservedip for detail.","title":"CRDs"},{"location":"concepts/config/","text":"Configuration Instructions for global configuration and environment args of Spiderpool. IPAM Plugin Configuration There is an example of IPAM configuration. { \"cniVersion\":\"0.3.1\", \"name\":\"macvlan-pod-network\", \"plugins\":[ { \"name\":\"macvlan-pod-network\", \"type\":\"macvlan\", \"master\":\"ens256\", \"mode\":\"bridge\", \"mtu\":1500, \"ipam\":{ \"type\":\"spiderpool\", \"log_file_path\":\"/var/log/spidernet/spiderpool.log\", \"log_file_max_size\":\"100M\", \"log_file_max_age\":\"30d\", \"log_file_max_count\":7, \"log_level\":\"INFO\" } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100M\" . log_file_max_age (string, optional): Max age of each rotated file, default to \"30d\" . log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" . Configmap Configuration Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock networkMode: legacy enableIPv4: true enableIPv6: true clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handle IPAM requests from IPAM plugin. networkMode : legacy : Applicable to the traditional physical machine network. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. clusterDefaultIPv4IPPool (array): Global default IPv4 ippools. It takes effect throughout the cluster. clusterDefaultIPv6IPPool (array): Global default IPv6 ippools. It takes effect throughout the cluster. Spiderpool-agent env env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Whether to enable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listen on , set to empty to disable it. SPIDERPOOL_UPDATE_CR_MAX_RETRYS 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide. Spiderpool-controller env env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Whether to enable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listen on , set to empty to disable it.","title":"Configuration"},{"location":"concepts/config/#configuration","text":"Instructions for global configuration and environment args of Spiderpool.","title":"Configuration"},{"location":"concepts/config/#ipam-plugin-configuration","text":"There is an example of IPAM configuration. { \"cniVersion\":\"0.3.1\", \"name\":\"macvlan-pod-network\", \"plugins\":[ { \"name\":\"macvlan-pod-network\", \"type\":\"macvlan\", \"master\":\"ens256\", \"mode\":\"bridge\", \"mtu\":1500, \"ipam\":{ \"type\":\"spiderpool\", \"log_file_path\":\"/var/log/spidernet/spiderpool.log\", \"log_file_max_size\":\"100M\", \"log_file_max_age\":\"30d\", \"log_file_max_count\":7, \"log_level\":\"INFO\" } } ] } log_file_path (string, optional): Path to log file of IPAM plugin, default to \"/var/log/spidernet/spiderpool.log\" . log_file_max_size (string, optional): Max size of each rotated file, default to \"100M\" . log_file_max_age (string, optional): Max age of each rotated file, default to \"30d\" . log_file_max_count (string, optional): Max number of rotated file, default to \"7\" . log_level (string, optional): Log level, default to \"INFO\" . It could be \"INFO\" , \"DEBUG\" , \"WARN\" , \"ERROR\" .","title":"IPAM Plugin Configuration"},{"location":"concepts/config/#configmap-configuration","text":"Configmap \"spiderpool-conf\" is the global configuration of Spiderpool. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: ipamUnixSocketPath: /var/run/spidernet/spiderpool.sock networkMode: legacy enableIPv4: true enableIPv6: true clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] ipamUnixSocketPath (string): Spiderpool agent listens to this UNIX socket file and handle IPAM requests from IPAM plugin. networkMode : legacy : Applicable to the traditional physical machine network. enableIPv4 (bool): true : Enable IPv4 IP allocation capability of Spiderpool. false : Disable IPv4 IP allocation capability of Spiderpool. enableIPv6 (bool): true : Enable IPv6 IP allocation capability of Spiderpool. false : Disable IPv6 IP allocation capability of Spiderpool. clusterDefaultIPv4IPPool (array): Global default IPv4 ippools. It takes effect throughout the cluster. clusterDefaultIPv6IPPool (array): Global default IPv6 ippools. It takes effect throughout the cluster.","title":"Configmap Configuration"},{"location":"concepts/config/#spiderpool-agent-env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Whether to enable metrics. SPIDERPOOL_HEALTH_PORT 5710 Metric HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5711 Spiderpool-agent backend HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5712 Port that gops is listen on , set to empty to disable it. SPIDERPOOL_UPDATE_CR_MAX_RETRYS 3 Max retries to update k8s resources. SPIDERPOOL_WORKLOADENDPOINT_MAX_HISTORY_RECORDS 100 Max historical IP allocation information allowed for a single Pod recorded in WorkloadEndpoint. SPIDERPOOL_IPPOOL_MAX_ALLOCATED_IPS 5000 Max number of IP that a single IP pool can provide.","title":"Spiderpool-agent env"},{"location":"concepts/config/#spiderpool-controller-env","text":"env default description SPIDERPOOL_LOG_LEVEL info Log level, optional values are \"debug\", \"info\", \"warn\", \"error\", \"fatal\", \"panic\". SPIDERPOOL_ENABLED_METRIC false Whether to enable metrics. SPIDERPOOL_HEALTH_PORT 5720 Spiderpool-controller backend HTTP server port. SPIDERPOOL_METRIC_HTTP_PORT 5721 Metric HTTP server port. SPIDERPOOL_WEBHOOK_PORT 5722 Webhook HTTP server port. SPIDERPOOL_CLI_PORT 5723 Spiderpool-CLI HTTP server port. SPIDERPOOL_GOPS_LISTEN_PORT 5724 Port that gops is listen on , set to empty to disable it.","title":"Spiderpool-controller env"},{"location":"concepts/gc/","text":"Resource Reclaim IP collection context When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still recorded to be used by a non-existed pod. when some error happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fail to call CNI plugin. A node is breakdown and then always not recovery, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing CNI binary on host when pod deletion. This issue will make bad result: the new pod maybe fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual pod number does not grow. Some CNI or IPAM plugins, they could not handle this issue. For some CNI, the administrator self needs to find issue IP and use CLI tool to reclaim them. For some CNI, it runs an interval job to find the issue IP and not reclaim them in time. For some CNI, there is not any mechanism at all, to fix these issue IP. solution For some CNI, its IP CIDR is big enough, so the leaked IP issue is not urgent. For spiderpool, all IP resource is managed by Administrator, and application will be bound to fixed IP, so the IP reclaim must be finished in time. The spiderpool controller takes charge of this responsibility. There is some environment to set its reclaim behaves. // TODO (Icarus9913), describe the environment ippool collection To prevent IP from leaking when ippool resource is deleted, the spiderpool has some rules: For an ippool, if there is still IP taken by pods, the spiderpool uses webhook to reject deleting request of ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool is set a finalizer by the spiderpool controller. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IP in the ippool is free.","title":"Resource Reclaim"},{"location":"concepts/gc/#resource-reclaim","text":"","title":"Resource Reclaim"},{"location":"concepts/gc/#ip-collection","text":"","title":"IP collection"},{"location":"concepts/gc/#context","text":"When a pod is normally deleted, the CNI plugin will be called to clean IP on a pod interface and make IP free on IPAM database. This can make sure all IPs are managed correctly and no IP leakage issue occurs. But on cases, it may go wrong and IP of IPAM database is still recorded to be used by a non-existed pod. when some error happened, the CNI plugin is not called correctly when pod deletion. This could happen like cases: When CNI plugin is called, its network communication goes wrong and fails to release IP. The container runtime goes wrong and fail to call CNI plugin. A node is breakdown and then always not recovery, the api-server makes pods of the breakdown node to be deleting status, but the CNI plugin fails to be called. BTW, this fault could be simply simulated by removing CNI binary on host when pod deletion. This issue will make bad result: the new pod maybe fail to run because the expected IP is still occupied. the IP resource is exhausted gradually although the actual pod number does not grow. Some CNI or IPAM plugins, they could not handle this issue. For some CNI, the administrator self needs to find issue IP and use CLI tool to reclaim them. For some CNI, it runs an interval job to find the issue IP and not reclaim them in time. For some CNI, there is not any mechanism at all, to fix these issue IP.","title":"context"},{"location":"concepts/gc/#solution","text":"For some CNI, its IP CIDR is big enough, so the leaked IP issue is not urgent. For spiderpool, all IP resource is managed by Administrator, and application will be bound to fixed IP, so the IP reclaim must be finished in time. The spiderpool controller takes charge of this responsibility. There is some environment to set its reclaim behaves. // TODO (Icarus9913), describe the environment","title":"solution"},{"location":"concepts/gc/#ippool-collection","text":"To prevent IP from leaking when ippool resource is deleted, the spiderpool has some rules: For an ippool, if there is still IP taken by pods, the spiderpool uses webhook to reject deleting request of ippool resource. For a deleting ippool, the IPAM plugin will stop assigning IP from it, but could release IP from it. The ippool is set a finalizer by the spiderpool controller. After the ippool goes to be deleting status, the spiderpool controller will remove the finalizer when all IP in the ippool is free.","title":"ippool collection"},{"location":"concepts/ippool/","text":"ippool // TODO (iiiceoo), describe the CRD","title":"ippool"},{"location":"concepts/ippool/#ippool","text":"// TODO (iiiceoo), describe the CRD","title":"ippool"},{"location":"concepts/metrics/","text":"Metric the spiderpool provides reach metrics spiderpool controller the metric of spiderpool controller is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool controller provides following metrics Metric Name Description spiderpool agent the metric of spiderpool agent is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool agent provides following metrics Metric Name Description","title":"Metric"},{"location":"concepts/metrics/#metric","text":"the spiderpool provides reach metrics","title":"Metric"},{"location":"concepts/metrics/#spiderpool-controller","text":"the metric of spiderpool controller is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool controller provides following metrics Metric Name Description","title":"spiderpool controller"},{"location":"concepts/metrics/#spiderpool-agent","text":"the metric of spiderpool agent is set by following pod environment environment description default SPIDERPOOL_ENABLED_METRIC enable metrics false SPIDERPOOL_METRIC_HTTP_PORT metrics port 5721 the spiderpool agent provides following metrics Metric Name Description","title":"spiderpool agent"},{"location":"concepts/reservedip/","text":"reservedip // todo(Icarus9913), describe the CRD","title":"reservedip"},{"location":"concepts/reservedip/#reservedip","text":"// todo(Icarus9913), describe the CRD","title":"reservedip"},{"location":"concepts/roadmap/","text":"spiderpool road map feature [ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference goal of April [x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5 goal of May [ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ?? goal of June [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a goal of July [ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc goal of August [ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 % goal of September [ ] start spiderflat","title":"spiderpool road map"},{"location":"concepts/roadmap/#spiderpool-road-map","text":"","title":"spiderpool road map"},{"location":"concepts/roadmap/#feature","text":"[ ] (alpha) a pod use multiple ippools, in case that an ippool is out of use [ ] (alpha) cooperate with macvlan [ ] (alpha) support pod / namespace annotations to customize ip [ ] (alpha) dual stack support. For ipv4-only, ipv6-only, dual-stack cluster, [ ] assign ipv4/ipv6 ip to pod [ ] all component could service on ipv4/ipv6 ip [ ] (alpha) ippool selector [ ] select namespace. Each namespace could occupy non-shared ippool [ ] select pod. For deployment and statefulset case, pod could occupy some ip range [ ] select node. For different zone, pod have specified ip range [ ] (beta) cooperate with multus. multus may call multiple CNI to assign different interface and assign ip [ ] (beta) fixed ip for application, especially for statefulset [ ] (beta) health check [ ] (beta) CLI for debug [ ] (beta) retrieve leaked ip [ ] for graceful timeout of terminating state [ ] for finished job [ ] trigger by hand or automatically [ ] trigger by pod event and happen at interval [ ] (beta) DCE5 integration [ ] (GA) metrics [ ] (GA) reserved ip [ ] (GA) administrator edit ip safely, preventing from race with IPAM CNI, and avoid ip conflicting between ippools [ ] (GA) good performance [ ] take xxx second at most, to assign a ip [ ] take xxx second at most, to assign 1000 ip [ ] (GA) good reliability [ ] (GA) cooperate with spiderflat [ ] Unit-Test [ ] (alpha) 40% coverage at least [ ] (beta) 70% coverage at least [ ] (GA) 90% coverage at least [ ] e2e test [ ] (alpha) 30% coverage for test case of alpha feature [ ] (beta) 80% coverage for test case of beta/alpha feature [ ] (GA) 100% coverage for test case of all feature [ ] (GA) chaos test case, performance test case [ ] All CICD pipeline. nightly ci, auto release chart/image/release, code lint, doc lint, unitest, e2e test [X] (alpha) 80% CICD pipeline [ ] (GA) 100% CICD pipeline [ ] documentation [ ] (alpha) architecture, contributing [ ] (beta) concept, get started, configuration [ ] (GA) command reference","title":"feature"},{"location":"concepts/roadmap/#goal-of-april","text":"[x] \u7b2c\u4e00\u4e2a Helm Release\uff0c\u53ef\u4ee5\u4e00\u952e\u90e8\u7f72\uff08\u53ef\u6301\u7eed CICD\uff09 [x] \u8f93\u51fa Road Map\uff0c\u5b8c\u6210 6\u4e2a\u6708\u4ee5\u4e0a\u7684\u89c4\u5212 [x] Unit-Test \u8986\u76d6\u7387 10 % [x] \u7b2c\u4e00\u4e2a \u81ea\u52a8\u5316\u7684 e2e \u6d4b\u8bd5\uff08\u6bcf\u5929\u665a\u4e0a\u90fd\u8981\u81ea\u52a8\u8dd1\uff09 [x] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 10% [x] GitHub (Star) 100 [x] webhook \u7cbe\u901a\u53ca\u5206\u4eab [ ] go builder \u7684 SDK \u751f\u6210\uff0c\u6240\u6709\u7684 client \u90fd\u80fd\u5de5\u4f5c\uff0c\u751f\u6210 CRD yaml\u3002 \u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 openapi \u63a5\u53e3\u548c SDK \uff0c\u9a8c\u8bc1\u90fd\u80fd\u5de5\u4f5c\u3002\u80fd\u505a\u5230 CI \u81ea\u52a8\u5316 SDK \u6821\u9a8c [x] \u5b8c\u6210 spiderpool ipam plugin \uff0cagent\u3001 controller\u8fdb\u7a0b\u80fd\u591f\u8dd1 \uff0c\u786e\u4fdd helm apply \u80fd\u591f\u90e8\u7f72\uff08\u4e0d\u8981\u6c42\u80fd\u8dd1\u4e1a\u52a1\uff09 [x] \u642d\u5efa e2e \u6846\u67b6 [x] \u7cbe\u8bfb golang\u3001ginkgo\u3001\u5f00\u6e90\u9879\u76ee e2e\uff0c \u4ee5macvlan + whereabout \u5b8c\u6210\u7b2c\u4e00\u4e2ae2e\u6d4b\u8bd5","title":"goal of April"},{"location":"concepts/roadmap/#goal-of-may","text":"[ ] \u5b8c\u6210 IPAM plug in [ ] \u5b8c\u6210 Agent \u4e3b\u8981\u4ee3\u7801\uff0c\u80fd\u5206\u914d\u51fa ipv4/ipv6 \u5730\u5740 [ ] \u4ee5 macvlan + whereabout \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 50% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u5b8c\u6210 100% alpha doc [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100% [ ] 5 \u4e2a\u5916\u90e8\u53cd\u9988\u7528\u6237\uff0c10\u4e2a ISSUE ????? [ ] 3\u4e2a\u5916\u90e8\u8d21\u732e\u8005 ???? [ ] GitHub (Star) 200 ??? [ ] Unit-Test \u8986\u76d6\u7387 30% [ ] \u5f00\u653e\u7684 \u72ec\u7acb\u7f51\u7ad9 \u548c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ???? [ ] \u5f00\u53d1\u7684 \u72ec\u7acb\u7f51\u7ad9 QuickStart\uff0c \u4e0b\u8f7d\uff0c\u53cd\u9988\u3002 ??? [ ] Spider Pool e2e \u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1 (\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545\u3002(\u8bc4\u5ba1\u4f1a\u4e4b\u524d\uff0c\u5f00\u4f1areview) [ ] \u5145\u5206\u7684 e2e \u6d4b\u8bd5 \u8bbe\u8ba1(\u5305\u62ec \u81ea\u52a8\u5347\u7ea7\uff0c\u6027\u80fd\uff0c\u53ef\u9760\u6027\uff0c\u8001\u5316) \u7ed3\u5408\u57fa\u7ebf \u548c\u8fc7\u5f80\u7684 L3 \u4e8b\u6545 ??","title":"goal of May"},{"location":"concepts/roadmap/#goal-of-june","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 alpha feature \u7684\u4e1a\u52a1\u4ee3\u7801 [ ] \u4ee5 macvlan + spiderpool \u4e3a\u65b9\u6848\uff0c\u5b8c\u6210 100% alpha feature \u7684 E2E \u7528\u4f8b [ ] \u6536\u96c6 10\u4e2a\u4ee5\u4e0a alpha \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] Unit-Test \u8986\u76d6\u7387 80% [ ] \u6027\u80fd\u6d4b\u8bd5\uff0c\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c??? [ ] \u5145\u5206\u7684\u707e\u96be\u573a\u666f\u6d4b\u8bd5\u3002\u548c \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 ?? [ ] \u517c\u5bb9\u6027\u6d4b\u8bd5\u9a8c\u8bc1\uff08\u6d89\u53ca\u7684\u5f00\u6e90\u8f6f\u4ef6\uff1a\u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u6e90\u9879\u76ee \u548c\u5404\u4e2a\u516c\u6709\u4e91\u73af\u5883\uff09 [ ] \u5ba1\u89c6\u201cL3\u4e8b\u6545\u201d\uff0c\u786e\u8ba4\u4e0d\u4f1a\u51fa\u73b0\u4e4b\u524d\u53d1\u751f\u8fc7\u7684\u4e8b\u6545 [ ] \u707e\u96be\u6062\u590d\u6f14\u7ec3 ?? beta [ ] \u5b8c\u6210 \u548c KubeGrid \u7684 \u4f1a\u5e08 ?? beta [ ] \u89e3\u51b3\u6240\u6709\u91cd\u5927bug [ ] \u5408\u4f5c\u4f19\u4f34 1 \u4e2a\uff0c\u53cd\u9988\u7528\u6237 1\u4e2a","title":"goal of June"},{"location":"concepts/roadmap/#goal-of-july","text":"[ ] \u5b8c\u6210\u6240\u6709\u6ee1\u8db3 beta feature \u7684\u4ee3\u7801 [ ] \u5b8c\u6210 100% beta doc","title":"goal of July"},{"location":"concepts/roadmap/#goal-of-august","text":"[ ] finish most of GA feature , wait for spiderflat ready to debug [ ] \u6536\u96c6 10 \u4e2a\u4ee5\u4e0a beta \u7528\u6237\u7684\u53cd\u9988\uff0c\u5e76\u89e3\u51b3\u5176\u53cd\u9988\u7684\u95ee\u9898(\u521a\u6027) [ ] \u5c06\u6240\u6709\u5df2\u77e5 BUG \u89e3\u51b3 [ ] OpenSSF(\u5f00\u6e90\u6700\u4f73\u5b9e\u8df5) \u5b8c\u6210\u5ea6 100 %","title":"goal of August"},{"location":"concepts/roadmap/#goal-of-september","text":"[ ] start spiderflat","title":"goal of September"},{"location":"concepts/workloadendpoint/","text":"workloadendpoint // TODO(iiiceoo), describe the CRD","title":"workloadendpoint"},{"location":"concepts/workloadendpoint/#workloadendpoint","text":"// TODO(iiiceoo), describe the CRD","title":"workloadendpoint"},{"location":"develop/changelog/","text":"Changelog How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/changelog/#changelog","text":"How to automatically generate changelogs: All PRs should be labeled with \"pr/release/***\" and can be merged. When you add the label, the changelog will be created automatically. The changelog contents include: New Features: it includes all PRs labeled with \"pr/release/feature-new\" Changed Features: it includes all PRs labeled with \"pr/release/feature-changed\" Fixes: it includes all PRs labeled with \"pr/release/bug\" All historical commits within this version The changelog will be attached to Github RELEASE and submitted to /changelogs of branch 'github_pages'.","title":"Changelog"},{"location":"develop/pullrequest/","text":"Submit Pull Request A pull request will be checked by following workflow, which is required for merging. Action: your PR should be signed off When you commit your modification, add -s in your commit command: git commit -s Action: check yaml files If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml . Action: check golang source code It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label Action: check licenses Any golang or shell file should be licensed correctly. Action: check markdown file Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling . Action: lint yaml file If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml . Action: lint chart Action: lint openapi.yaml Action: check code spell Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Submit Pull Request"},{"location":"develop/pullrequest/#submit-pull-request","text":"A pull request will be checked by following workflow, which is required for merging.","title":"Submit Pull Request"},{"location":"develop/pullrequest/#action-your-pr-should-be-signed-off","text":"When you commit your modification, add -s in your commit command: git commit -s","title":"Action: your PR should be signed off"},{"location":"develop/pullrequest/#action-check-yaml-files","text":"If this check fails, see the yaml rule . Once the issue is fixed, it could be verified on your local host by command make lint-yaml . Note: To ignore a yaml rule, you can add it into .github/yamllint-conf.yml .","title":"Action: check yaml files"},{"location":"develop/pullrequest/#action-check-golang-source-code","text":"It checks the following items against any updated golang file. Mod dependency updated, golangci-lint, gofmt updated, go vet, use internal lock pkg Comment // TODO should follow the format: // TODO (AuthorName) ... , which easy to trace the owner of the remaining job Unitest and upload coverage to codecov Each golang test file should mark ginkgo label","title":"Action: check golang source code"},{"location":"develop/pullrequest/#action-check-licenses","text":"Any golang or shell file should be licensed correctly.","title":"Action: check licenses"},{"location":"develop/pullrequest/#action-check-markdown-file","text":"Check markdown format, if fails, See the Markdown Rule You can test it on your local machine with the command make lint-markdown-format . You can fix it on your local machine with the command make fix-markdown-format . If you believe it can be ignored, you can add it to .github/markdownlint.yaml . Check markdown spell error. You can test it with the command make lint-markdown-spell-colour . If you believe it can be ignored, you can add it to .github/.spelling .","title":"Action: check markdown file"},{"location":"develop/pullrequest/#action-lint-yaml-file","text":"If it fails, see https://yamllint.readthedocs.io/en/stable/rules.html for reasons. You can test it on your local machine with the command make lint-yaml .","title":"Action: lint yaml file"},{"location":"develop/pullrequest/#action-lint-chart","text":"","title":"Action: lint chart"},{"location":"develop/pullrequest/#action-lint-openapiyaml","text":"","title":"Action: lint openapi.yaml"},{"location":"develop/pullrequest/#action-check-code-spell","text":"Any code spell error of golang files will be checked. You can check it on your local machine with the command make lint-code-spell . It could be automatically fixed on your local machine with the command make fix-code-spell . If you believe it can be ignored, edit .github/codespell-ignorewords and make sure all letters are lower-case.","title":"Action: check code spell"},{"location":"develop/release/","text":"workflow for release If a tag is pushed , the following steps will run: build the images with the pushed tag, and push to ghcr registry generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". build the chart package with the pushed tag, and submit a PR to branch 'github_pages' submit '/docs' of branch 'main' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"","title":"workflow for release"},{"location":"develop/release/#workflow-for-release","text":"If a tag is pushed , the following steps will run: build the images with the pushed tag, and push to ghcr registry generate the changelog by historical PR labeled as \"pr/release/*\" submit the changelog file to branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\". build the chart package with the pushed tag, and submit a PR to branch 'github_pages' submit '/docs' of branch 'main' to '/docs' of branch 'github_pages' create a GitHub Release attached with the chart package and changelog Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"","title":"workflow for release"},{"location":"develop/swagger_openapi/","text":"SWAGGER OPENAPI Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes. Features Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs. Usages There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR . validate spec Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec generate source codes with the given spec To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen verify the spec with current source codes to make sure whether the current source codes is out of date To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify clean the generated source codes To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code Use swagger-ui To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web. Steps For Developers Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#swagger-openapi","text":"Spiderpool uses go-swagger to generate open api source codes. There are two swagger yaml for 'agent' and 'controller'. Please check with agent-swagger spec and controller-swagger spec . source codes.","title":"SWAGGER OPENAPI"},{"location":"develop/swagger_openapi/#features","text":"Validate spec Generate C/S codes Verify spec with current source codes Clean codes Use swagger-ui to analyze the given specs.","title":"Features"},{"location":"develop/swagger_openapi/#usages","text":"There are two ways for you to get access to the features. Use makefile , it's the simplest way. Use shell swag.sh . The format usage for 'swag.sh' is swag.sh $ACTION $SPEC_DIR .","title":"Usages"},{"location":"develop/swagger_openapi/#validate-spec","text":"Validate the current spec just give the second parameter with the spec directory. ./tools/scripts/swag.sh validate ./api/v1/agent Or you can use makefile to validate the spiderpool agent and controller with the following command. make openapi-validate-spec","title":"validate spec"},{"location":"develop/swagger_openapi/#generate-source-codes-with-the-given-spec","text":"To generate agent source codes: ./tools/scripts/swag.sh generate ./api/v1/agent Or you can use makefile to generate for both of agent and controller two: make openapi-code-gen","title":"generate source codes with the given spec"},{"location":"develop/swagger_openapi/#verify-the-spec-with-current-source-codes-to-make-sure-whether-the-current-source-codes-is-out-of-date","text":"To verify the given spec whether valid or not: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to verify for both of agent and controller two: make openapi-verify","title":"verify the spec with current source codes to make sure whether the current source codes is out of date"},{"location":"develop/swagger_openapi/#clean-the-generated-source-codes","text":"To clean the generated agent codes: ./tools/scripts/swag.sh verify ./api/v1/agent Or you can use makefile to clean for both of agent and controller two: make clean-openapi-code","title":"clean the generated source codes"},{"location":"develop/swagger_openapi/#use-swagger-ui","text":"To analyze the defined specs in your local environment with docker: make openapi-ui Then you can visit the web with port 8080. Switch the yaml with './agent-swagger.yaml' and './controller-swagger.yaml' in the web.","title":"Use swagger-ui"},{"location":"develop/swagger_openapi/#steps-for-developers","text":"Modify the specs: agent-swagger spec and controller-swagger spec Validate the modified specs Use swagger-ui to check the effects in your local environment with docker Re-generate the source codes with the modified specs Commit your PR.","title":"Steps For Developers"},{"location":"develop/test/","text":"test you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init -e E2E_IP_FAMILY=ipv6 run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test # run all e2e test on ipv4-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test 5 finally, you could visit \"http://HostIp:4040\" the in the browser of your desktop, and get flamegraph","title":"test"},{"location":"develop/test/#test","text":"you could follow the below steps to test: check required developing tools on you local host. If something missing, please run 'test/scripts/install-tools.sh' to install them # make dev-doctor go version go1.17 linux/amd64 check e2e tools pass 'docker' installed pass 'kubectl' installed pass 'kind' installed pass 'p2ctl' installed finish checking e2e tools run the e2e # make e2e if your run it for the first time, it will download some images, you could set the http proxy # ADDR=10.6.0.1 # export https_proxy=http://${ADDR}:7890 http_proxy=http://${ADDR}:7890 # make e2e run a specified case # make e2e -e E2E_GINKGO_LABELS=\"lable1,label2\" you could do it step by step with the follow build the image # do some coding $ git add . $ git commit -s -m 'message' # !!! images is built by commit sha, so make sure the commit is submit locally $ make build_image # or (if buildx fail to pull images) $ make build_docker_image setup the cluster # setup the kind cluster of dual-stack # !!! images is tested by commit sha, so make sure the commit is submit locally $ make e2e_init ....... ----------------------------------------------------------------------------------------------------- succeeded to setup cluster spider you could use following command to access the cluster export KUBECONFIG=/root/git/spiderpool/test/.cluster/spider/.kube/config kubectl get nodes ----------------------------------------------------------------------------------------------------- # setup the kind cluster of ipv4-only $ make e2e_init -e E2E_IP_FAMILY=ipv4 # setup the kind cluster of ipv6-only $ make e2e_init -e E2E_IP_FAMILY=ipv6 run the e2e test # run all e2e test on dual-stack cluster $ make e2e_test # run all e2e test on ipv4-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv4 # run all e2e test on ipv6-only cluster $ make e2e_test -e E2E_IP_FAMILY=ipv6 # run smoke test $ make e2e_test -e E2E_GINKGO_LABELS=smoke # after finishing e2e case , you could test repeated for debugging flaky tests # example: run a case repeatedly $ make e2e_test -e E2E_GINKGO_LABELS=CaseLabel -e GINKGO_OPTION=\"--repeat=10 \" # example: run a case until fails $ make e2e_test -e GINKGO_OPTION=\" --label-filter=CaseLabel --until-it-fails \" $ ls e2ereport.json $ make clean_e2e you could test specified images with the follow # load images to docker $ docker pull ${AGENT_IMAGE_NAME}:${IMAGE_TAG} $ docker pull ${CONTROLLER_IMAGE_NAME}:${IMAGE_TAG} # setup the cluster with the specified image $ make e2e_init -e TEST_IMAGE_TAG=${IMAGE_TAG} \\ -e SPIDERPOOL_AGENT_IMAGE_NAME=${AGENT_IMAGE_NAME} \\ -e SPIDERPOOL_CONTROLLER_IMAGE_NAME=${CONTROLLER_IMAGE_NAME} # run all e2e test $ make e2e_test 5 finally, you could visit \"http://HostIp:4040\" the in the browser of your desktop, and get flamegraph","title":"test"},{"location":"example/ippool-gc/","text":"","title":"Ippool gc"},{"location":"usage/basic/","text":"Basic Description Notice get started install Refer to installation create ippool create application get metrics clean up","title":"Basic"},{"location":"usage/basic/#basic","text":"","title":"Basic"},{"location":"usage/basic/#description","text":"","title":"Description"},{"location":"usage/basic/#notice","text":"","title":"Notice"},{"location":"usage/basic/#get-started","text":"","title":"get started"},{"location":"usage/basic/#install","text":"Refer to installation","title":"install"},{"location":"usage/basic/#create-ippool","text":"","title":"create ippool"},{"location":"usage/basic/#create-application","text":"","title":"create application"},{"location":"usage/basic/#get-metrics","text":"","title":"get metrics"},{"location":"usage/basic/#clean-up","text":"","title":"clean up"},{"location":"usage/cidrManager/","text":"","title":"cidrManager"},{"location":"usage/cli/","text":"spiderpoolctl the spiderpoolctl is CLI tool, help to debug the spiderpool","title":"spiderpoolctl"},{"location":"usage/cli/#spiderpoolctl","text":"the spiderpoolctl is CLI tool, help to debug the spiderpool","title":"spiderpoolctl"},{"location":"usage/debug/","text":"Q&A","title":"Q&A"},{"location":"usage/debug/#qa","text":"","title":"Q&amp;A"},{"location":"usage/install/","text":"Install the spiderpool needs install webhook of kube-apiserver, so it needs tls certificates. there are two ways to install it, the one is with cert-manager, the other one is to generate self-signed certificate. install spiderpool install By Self-signed Certificates this way is simple, there is no any dependency. The project provides a script to generate tls certificate the following is a ipv4-only example helm repo add spiderpool https://spidernet-io.github.io/spiderpool git clone https://github.com/spidernet-io/spiderpool.git cd spiderpool # generate the certificates tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` # for default ipv4 ippool # CIDR Ipv4Subnet=\"172.20.0.0/16\" # available IP resource Ipv4Range=\"172.20.0.10-172.20.0.200\" # deploy the spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.provided.tlsKey=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.provided.tlsCa=\"${CA}\" \\ --set feature.enableIPv4=true --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${Ipv4Subnet} --set clusterDefaultPool.ipv4IPRanges={${Ipv4Range}} the following is a dual-stack example helm repo add spiderpool https://spidernet-io.github.io/spiderpool # generate the certificates tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` # for default ipv4 ippool # CIDR Ipv4Subnet=\"172.20.0.0/16\" # available IP resource Ipv4Range=\"172.20.0.10-172.20.0.200\" # for default ipv6 ippool # CIDR Ipv6Subnet=\"fd00::/112\" # available IP resource Ipv6Range=\"fd00::10-fd00::200\" # deploy the spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.provided.tlsKey=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.provided.tlsCa=\"${CA}\" \\ --set feature.enableIPv4=true --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv4IPPool=true --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${Ipv4Subnet} --set clusterDefaultPool.ipv4IPRanges={${Ipv4Range}} \\ --set clusterDefaultPool.ipv6Subnet=${Ipv6Subnet} --set clusterDefaultPool.ipv6IPRanges={${Ipv6Range}} NOTICE spiderpool-controller pod is running as hostnetwork mode, and it needs take host port, it is set with podAntiAffinity to make sure that a node will only run a spiderpool-controller pod. so, if you set the replicas number of spiderpool-controller bigger than 2, make sure there is enough nodes install By Cert-manager the way is not a common situation, because cert-manager needs CNI to create its pod, but as IPAM, spiderpool is still not installed to provide IP resource. It means cert-manager and spiderpool need each other to finish installation. Therefore, the way may implement on following situation: after spiderpool is installed by self-signed certificates, and the cert-manager is deployed, then it could change to cert-manager scheme on cluster with Multus CNI , the cert-manager pods is deployed by other CNI, then spiderpool could be deployed by cert-manager helm repo add spiderpool https://spidernet-io.github.io/spiderpool # for default ipv4 ippool # CIDR ipv4_subnet=\"172.20.0.0/16\" # available IP resource ipv4_range=\"172.20.0.10-172.20.0.200\" helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=certmanager \\ --set spiderpoolController.tls.certmanager.issuerName=${CERT_MANAGER_ISSUER_NAME} \\ --set feature.enableIPv4=true --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true --set clusterDefaultPool.installIPv6IPPool=false \\ --set clusterDefaultPool.ipv4Subnet=${ipv4_subnet} \\ --set clusterDefaultPool.ipv4IPRanges={${ipv4_ip_range}} configure CNI after installation of the spiderpool, please edit CNI configuration file under /etc/cni/net.d/ . The following is an example for macvlan CNI { \"cniVersion\": \"0.3.1\", \"type\": \"macvlan\", \"mode\": \"bridge\", \"master\": \"eth0\", \"name\": \"macvlan-cni-default\", \"ipam\": { \"type\": \"spiderpool\" } } you cloud refer config for the detail of the IPAM configuration","title":"Install"},{"location":"usage/install/#install","text":"the spiderpool needs install webhook of kube-apiserver, so it needs tls certificates. there are two ways to install it, the one is with cert-manager, the other one is to generate self-signed certificate.","title":"Install"},{"location":"usage/install/#install-spiderpool","text":"","title":"install spiderpool"},{"location":"usage/install/#install-by-self-signed-certificates","text":"this way is simple, there is no any dependency. The project provides a script to generate tls certificate the following is a ipv4-only example helm repo add spiderpool https://spidernet-io.github.io/spiderpool git clone https://github.com/spidernet-io/spiderpool.git cd spiderpool # generate the certificates tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` # for default ipv4 ippool # CIDR Ipv4Subnet=\"172.20.0.0/16\" # available IP resource Ipv4Range=\"172.20.0.10-172.20.0.200\" # deploy the spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.provided.tlsKey=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.provided.tlsCa=\"${CA}\" \\ --set feature.enableIPv4=true --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${Ipv4Subnet} --set clusterDefaultPool.ipv4IPRanges={${Ipv4Range}} the following is a dual-stack example helm repo add spiderpool https://spidernet-io.github.io/spiderpool # generate the certificates tools/cert/generateCert.sh \"/tmp/tls\" CA=`cat /tmp/tls/ca.crt | base64 -w0 | tr -d '\\n' ` SERVER_CERT=` cat /tmp/tls/server.crt | base64 -w0 | tr -d '\\n' ` SERVER_KEY=` cat /tmp/tls/server.key | base64 -w0 | tr -d '\\n' ` # for default ipv4 ippool # CIDR Ipv4Subnet=\"172.20.0.0/16\" # available IP resource Ipv4Range=\"172.20.0.10-172.20.0.200\" # for default ipv6 ippool # CIDR Ipv6Subnet=\"fd00::/112\" # available IP resource Ipv6Range=\"fd00::10-fd00::200\" # deploy the spiderpool helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=provided \\ --set spiderpoolController.tls.provided.tlsCert=\"${SERVER_CERT}\" \\ --set spiderpoolController.tls.provided.tlsKey=\"${SERVER_KEY}\" \\ --set spiderpoolController.tls.provided.tlsCa=\"${CA}\" \\ --set feature.enableIPv4=true --set feature.enableIPv6=true \\ --set clusterDefaultPool.installIPv4IPPool=true --set clusterDefaultPool.installIPv6IPPool=true \\ --set clusterDefaultPool.ipv4Subnet=${Ipv4Subnet} --set clusterDefaultPool.ipv4IPRanges={${Ipv4Range}} \\ --set clusterDefaultPool.ipv6Subnet=${Ipv6Subnet} --set clusterDefaultPool.ipv6IPRanges={${Ipv6Range}} NOTICE spiderpool-controller pod is running as hostnetwork mode, and it needs take host port, it is set with podAntiAffinity to make sure that a node will only run a spiderpool-controller pod. so, if you set the replicas number of spiderpool-controller bigger than 2, make sure there is enough nodes","title":"install By Self-signed Certificates"},{"location":"usage/install/#install-by-cert-manager","text":"the way is not a common situation, because cert-manager needs CNI to create its pod, but as IPAM, spiderpool is still not installed to provide IP resource. It means cert-manager and spiderpool need each other to finish installation. Therefore, the way may implement on following situation: after spiderpool is installed by self-signed certificates, and the cert-manager is deployed, then it could change to cert-manager scheme on cluster with Multus CNI , the cert-manager pods is deployed by other CNI, then spiderpool could be deployed by cert-manager helm repo add spiderpool https://spidernet-io.github.io/spiderpool # for default ipv4 ippool # CIDR ipv4_subnet=\"172.20.0.0/16\" # available IP resource ipv4_range=\"172.20.0.10-172.20.0.200\" helm install spiderpool spiderpool/spiderpool --namespace kube-system \\ --set spiderpoolController.tls.method=certmanager \\ --set spiderpoolController.tls.certmanager.issuerName=${CERT_MANAGER_ISSUER_NAME} \\ --set feature.enableIPv4=true --set feature.enableIPv6=false \\ --set clusterDefaultPool.installIPv4IPPool=true --set clusterDefaultPool.installIPv6IPPool=false \\ --set clusterDefaultPool.ipv4Subnet=${ipv4_subnet} \\ --set clusterDefaultPool.ipv4IPRanges={${ipv4_ip_range}}","title":"install By Cert-manager"},{"location":"usage/install/#configure-cni","text":"after installation of the spiderpool, please edit CNI configuration file under /etc/cni/net.d/ . The following is an example for macvlan CNI { \"cniVersion\": \"0.3.1\", \"type\": \"macvlan\", \"mode\": \"bridge\", \"master\": \"eth0\", \"name\": \"macvlan-cni-default\", \"ipam\": { \"type\": \"spiderpool\" } } you cloud refer config for the detail of the IPAM configuration","title":"configure CNI"},{"location":"usage/ippool-affinity-namespace/","text":"","title":"Ippool affinity namespace"},{"location":"usage/ippool-affinity-node/","text":"","title":"Ippool affinity node"},{"location":"usage/ippool-affinity-pod/","text":"pod affinity of ippool Spiderpool supports multiple ways to select ippool. Pod will select a specific ippool to allocate IP according to the corresponding rules that with different priorities. Meanwhile, ippool can use selector to filter its user. Priority Spiderpool supports the following ways to specify ippool for Pod: Pod annotations ( high priority ): Specifies which ippool the current Pod should use to allocate IP, which overrides any other selection rules. ipam.spidernet.io/ippool : For single interface case. Ensure that the interface field specified in Pod annotations is consistent with that in the CNI request. yaml ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\", \"v6-ippool2\"] } ipam.spidernet.io/ippools : For multiple interfaces case. Note that it does not means that CNI will return the IP allocation results of multiple interfaces in one request (this will break the CNI Specification ). Spiderpool will allocate multiple IP in one request, but return them in several times. It is mainly used with Spiderflat CNI and multus CNI . You can get more details of \"multus + Spiderpool\" here . yaml ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\"], \"defaultRoute\": true },{ \"interface\": \"eth1\", \"ipv4pools\": [\"v4-ippool2\"], \"ipv6pools\": [\"v6-ippool2\"], \"defaultRoute\": false }] BTW, ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . Namespace annotations ( medium priority ): Specifies default ippools for current Namespace. Pods that do not have Pod IPAM annotations under this Namespace will all be allocated IP by these ippools. ipam.spidernet.io/defaultv4ippool : Default IPv4 ippools at Namespace level. yaml ipam.spidernet.io/defaultv4ippool: '[\"v4-ippool1\",\"v4-ippool2\"]' ipam.spidernet.io/defaultv6ippool : Default IPv6 ippools at Namespace level. yaml ipam.spidernet.io/defaultv6ippool: '[\"v6-ippool1\",\"v6-ippool2\"]' Configmap ( low priority )\uff1aDefault ippools at cluster level. Pods that do not append any special ippool selection rules will try to allocate IP from these ippools. They are valid throughout the cluster. yaml apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] ... More details of Spiderpool annotation and configuration . Backup Each ippool selection rule supports 'backup'. We can specify multiple ippools in the array to achieve this effect. Spiderpool will successively try to allocate IP in the order of the elements in the ippool array until the first allocation succeeds or all fail. The following is an example of Pod annotation ipam.spidernet.io/ippool , the same is true for other ippool selection rules. We can create such a Pod: apiVersion: v1 kind: Pod metadata: name: sample-backup annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"default-v4-ippool\", \"backup-v4-ippool\"] } spec: containers: - name: sample-backup image: alpine imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As we can see from the above, Pod sample-backup will attempt to allocate IP from ippools defined in field ipv4pools . Unfortunately, ippool default-v4-ippool has run out of IP. $ kubectl get spl NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE default-v4-ippool IPv4 172.18.0.0/16 5 5 false backup-v4-ippool IPv4 172.18.0.0/16 1 5 false We will see Pod sample-backup successfully allocated IP from ippool backup-v4-ippool . $ kubectl get swe sample-backup -n default NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME sample-backup eth0 backup-v4-ippool 172.18.40.40/16 spider-worker 1m33s Selectors Ippool can also use nodeSelector , namesapceSelector , and podSelector to filter its users. It should be regarded as a filtering mechanism rather than an ippool selection rule (refer to chapter Priority for ippool selection rules). All selectors follow the syntax of Kubernetes label selector , just like: selector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} We have such an ippool: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: default-v4-ippool spec: disable: false ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.40.41-172.18.40.50 vlan: 0 namesapceSelector: matchLabels: foo: bar It means that only Pod under the Namespace with foo: bar label can use this ippool. At the same time, default-v4-ippool is also the default IPv4 ippool of the cluster. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: enableIPv4: true enableIPv6: false clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [] ... Try to create a Deployment in default Namespace. $ kubectl create deployment my-deploy --image=nginx However, this Deployment will eventually fail to function properly because it is not allocated any IP, and not in a Namespace that matches ippool's namespaceSelector . So don't give too harsh selectors to cluster default ippools , which will not make life better :) Example After understanding what selectors' \"filtering mechanism\" is, let's take a look at how selectors work with ippool selection rules. There are two ippools: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: master-v4-ippool spec: ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.50.41-172.18.50.50 nodeSelector: matchExpressions: - {key: node-role.kubernetes.io/master, operator: Exists} Obviously, ippool master-v4-ippool only works with the control plane nodes of Kubernetes. The Pod which scheduled to the master nodes can be correctly allocated IP from this ippool. apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: worker-v4-ippool spec: ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.50.51-172.18.50.60 nodeSelector: matchExpressions: - {key: node-role.kubernetes.io/master, operator: DoesNotExist} ippool worker-v4-ippool is the opposite. Then, we run the following Deployment example , which has 5 replicas. We expect that the Pods controlled by example can be evenly scheduled to master nodes and worker nodes. Of course, maybe you should remove some related Taints. apiVersion: apps/v1 kind: Deployment metadata: name: example spec: replicas: 5 selector: matchLabels: app: example template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"master-v4-ippool\", \"worker-v4-ippool\"] } labels: app: example spec: containers: - image: alpine imagePullPolicy: IfNotPresent name: example command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Finally, we will find that Pods at different nodes use different ippools. $ kubectl get swe -n default NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME example-554cc84db6-kr8j5 eth0 master-v4-ippool 172.18.50.47/16 control-plane 3s example-554cc84db6-lkdbz eth0 worker-v4-ippool 172.18.50.51/16 worker 4s example-554cc84db6-qbmwv eth0 worker-v4-ippool 172.18.50.58/16 worker 3s example-554cc84db6-r6qpt eth0 worker-v4-ippool 172.18.50.55/16 worker 4s example-554cc84db6-rjstk eth0 master-v4-ippool 172.18.50.43/16 control-plane 4s","title":"pod affinity of ippool"},{"location":"usage/ippool-affinity-pod/#pod-affinity-of-ippool","text":"Spiderpool supports multiple ways to select ippool. Pod will select a specific ippool to allocate IP according to the corresponding rules that with different priorities. Meanwhile, ippool can use selector to filter its user.","title":"pod affinity of ippool"},{"location":"usage/ippool-affinity-pod/#priority","text":"Spiderpool supports the following ways to specify ippool for Pod: Pod annotations ( high priority ): Specifies which ippool the current Pod should use to allocate IP, which overrides any other selection rules. ipam.spidernet.io/ippool : For single interface case. Ensure that the interface field specified in Pod annotations is consistent with that in the CNI request. yaml ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\", \"v6-ippool2\"] } ipam.spidernet.io/ippools : For multiple interfaces case. Note that it does not means that CNI will return the IP allocation results of multiple interfaces in one request (this will break the CNI Specification ). Spiderpool will allocate multiple IP in one request, but return them in several times. It is mainly used with Spiderflat CNI and multus CNI . You can get more details of \"multus + Spiderpool\" here . yaml ipam.spidernet.io/ippools: |- [{ \"interface\": \"eth0\", \"ipv4pools\": [\"v4-ippool1\"], \"ipv6pools\": [\"v6-ippool1\"], \"defaultRoute\": true },{ \"interface\": \"eth1\", \"ipv4pools\": [\"v4-ippool2\"], \"ipv6pools\": [\"v6-ippool2\"], \"defaultRoute\": false }] BTW, ipam.spidernet.io/ippools has precedence over ipam.spidernet.io/ippool . Namespace annotations ( medium priority ): Specifies default ippools for current Namespace. Pods that do not have Pod IPAM annotations under this Namespace will all be allocated IP by these ippools. ipam.spidernet.io/defaultv4ippool : Default IPv4 ippools at Namespace level. yaml ipam.spidernet.io/defaultv4ippool: '[\"v4-ippool1\",\"v4-ippool2\"]' ipam.spidernet.io/defaultv6ippool : Default IPv6 ippools at Namespace level. yaml ipam.spidernet.io/defaultv6ippool: '[\"v6-ippool1\",\"v6-ippool2\"]' Configmap ( low priority )\uff1aDefault ippools at cluster level. Pods that do not append any special ippool selection rules will try to allocate IP from these ippools. They are valid throughout the cluster. yaml apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [default-v6-ippool] ... More details of Spiderpool annotation and configuration .","title":"Priority"},{"location":"usage/ippool-affinity-pod/#backup","text":"Each ippool selection rule supports 'backup'. We can specify multiple ippools in the array to achieve this effect. Spiderpool will successively try to allocate IP in the order of the elements in the ippool array until the first allocation succeeds or all fail. The following is an example of Pod annotation ipam.spidernet.io/ippool , the same is true for other ippool selection rules. We can create such a Pod: apiVersion: v1 kind: Pod metadata: name: sample-backup annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"default-v4-ippool\", \"backup-v4-ippool\"] } spec: containers: - name: sample-backup image: alpine imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] As we can see from the above, Pod sample-backup will attempt to allocate IP from ippools defined in field ipv4pools . Unfortunately, ippool default-v4-ippool has run out of IP. $ kubectl get spl NAME VERSION SUBNET ALLOCATED-IP-COUNT TOTAL-IP-COUNT DISABLE default-v4-ippool IPv4 172.18.0.0/16 5 5 false backup-v4-ippool IPv4 172.18.0.0/16 1 5 false We will see Pod sample-backup successfully allocated IP from ippool backup-v4-ippool . $ kubectl get swe sample-backup -n default NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME sample-backup eth0 backup-v4-ippool 172.18.40.40/16 spider-worker 1m33s","title":"Backup"},{"location":"usage/ippool-affinity-pod/#selectors","text":"Ippool can also use nodeSelector , namesapceSelector , and podSelector to filter its users. It should be regarded as a filtering mechanism rather than an ippool selection rule (refer to chapter Priority for ippool selection rules). All selectors follow the syntax of Kubernetes label selector , just like: selector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} We have such an ippool: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: default-v4-ippool spec: disable: false ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.40.41-172.18.40.50 vlan: 0 namesapceSelector: matchLabels: foo: bar It means that only Pod under the Namespace with foo: bar label can use this ippool. At the same time, default-v4-ippool is also the default IPv4 ippool of the cluster. apiVersion: v1 kind: ConfigMap metadata: name: spiderpool-conf namespace: kube-system data: enableIPv4: true enableIPv6: false clusterDefaultIPv4IPPool: [default-v4-ippool] clusterDefaultIPv6IPPool: [] ... Try to create a Deployment in default Namespace. $ kubectl create deployment my-deploy --image=nginx However, this Deployment will eventually fail to function properly because it is not allocated any IP, and not in a Namespace that matches ippool's namespaceSelector . So don't give too harsh selectors to cluster default ippools , which will not make life better :)","title":"Selectors"},{"location":"usage/ippool-affinity-pod/#example","text":"After understanding what selectors' \"filtering mechanism\" is, let's take a look at how selectors work with ippool selection rules. There are two ippools: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: master-v4-ippool spec: ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.50.41-172.18.50.50 nodeSelector: matchExpressions: - {key: node-role.kubernetes.io/master, operator: Exists} Obviously, ippool master-v4-ippool only works with the control plane nodes of Kubernetes. The Pod which scheduled to the master nodes can be correctly allocated IP from this ippool. apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: worker-v4-ippool spec: ipVersion: IPv4 subnet: 172.18.0.0/16 ips: - 172.18.50.51-172.18.50.60 nodeSelector: matchExpressions: - {key: node-role.kubernetes.io/master, operator: DoesNotExist} ippool worker-v4-ippool is the opposite. Then, we run the following Deployment example , which has 5 replicas. We expect that the Pods controlled by example can be evenly scheduled to master nodes and worker nodes. Of course, maybe you should remove some related Taints. apiVersion: apps/v1 kind: Deployment metadata: name: example spec: replicas: 5 selector: matchLabels: app: example template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"master-v4-ippool\", \"worker-v4-ippool\"] } labels: app: example spec: containers: - image: alpine imagePullPolicy: IfNotPresent name: example command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] Finally, we will find that Pods at different nodes use different ippools. $ kubectl get swe -n default NAME INTERFACE IPV4POOL IPV4 IPV6POOL IPV6 NODE CREATETION TIME example-554cc84db6-kr8j5 eth0 master-v4-ippool 172.18.50.47/16 control-plane 3s example-554cc84db6-lkdbz eth0 worker-v4-ippool 172.18.50.51/16 worker 4s example-554cc84db6-qbmwv eth0 worker-v4-ippool 172.18.50.58/16 worker 3s example-554cc84db6-r6qpt eth0 worker-v4-ippool 172.18.50.55/16 worker 4s example-554cc84db6-rjstk eth0 master-v4-ippool 172.18.50.43/16 control-plane 4s","title":"Example"},{"location":"usage/ippool-multi/","text":"multiple ippool set multiple ippool for backup usage","title":"multiple ippool"},{"location":"usage/ippool-multi/#multiple-ippool","text":"set multiple ippool for backup usage","title":"multiple ippool"},{"location":"usage/ippool-namesapce/","text":"","title":"Ippool namesapce"},{"location":"usage/ipv6/","text":"","title":"Ipv6"},{"location":"usage/multi-interfaces-annotation/","text":"","title":"Multi interfaces annotation"},{"location":"usage/multi-interfaces-cni/","text":"","title":"Multi interfaces cni"},{"location":"usage/reserved-ip/","text":"Reserved IP Spiderpool reserve some IP addresses for the whole cluster, which will not be used by any IPAM allocation. Usually, these IP addresses are some external IP addresses. IPPool excludeIPs First of all, you may have observed that there is an excludeIPs field in IPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main work is not so. Honestly, excludeIPs field is more of a syntax sugar , so that users can define their IPPool CRD more flexibly. For example, now we want to create an IPPool, which contains two IP ranges: 172.18.40.40-172.18.40.44 and 172.18.40.46-172.18.40.50 . Without using excludeIPs , we may need the following IPPool manifest: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: not-use-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.44 - 172.18.40.46-172.18.40.50 But in fact, we can more concisely describe this semantics through excludeIPs : apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: use-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.50 excludeIPs: - 172.18.40.45 Of course, ExcludeIPs also supports the format of IP range and we can define multiple excludeIPs records to segment a subnet in more detail: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IP-ranges-in-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.50 excludeIPs: - 172.18.40.45 - 172.18.40.47-172.18.40.49 excludeIPs will make sure that any Pod that allocates IP from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined. ReservedIP Unlike excluedIPs field in IPPool CRD, ReservedIP CRD is actually used to define the global reserved IP rules of a cluster. ReservedIP ensures that no Pod in a cluster will use these IP addresses defined by it, whether or not some IPPools have inadvertently defined the same IP addresses for Pods use. For example, we have such an IPPool with 10 IP addresses from 172.18.50.40 to 172.18.50.50 : apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: test-IPv4-IP-pool spec: ipVersion: 4 subnet: 172.18.50.0/24 ips: - 172.18.50.40-172.18.50.50 Unfortunately, a ReservedIP has been pre created in cluster, which reserves 100 IP addresses from 172.18.50.1 to 172.18.50.100 : apiVersion: spiderpool.spidernet.io/v1 kind: ReservedIP metadata: name: reserved spec: ipVersion: 4 ips: - 172.18.50.1-172.18.50.100 Now, if we create a Deployment and let its Pods allocate IP addresses from IPPool test-IPv4-IP-pool : apiVersion: apps/v1 kind: Deployment metadata: name: example spec: replicas: 3 selector: matchLabels: app: example template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"test-IPv4-IP-pool\"], } labels: app: example spec: containers: - image: alpine imagePullPolicy: IfNotPresent name: example command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] None of these Pods can run successfully because \"all IP addresses are used out\". But when we remove this ReservedIP reserved , everything will return to normal. $ kubectl get po -l app=example NAMESPACE NAME READY STATUS RESTARTS AGE default example-6c5cdc6fb6-hvzrp 0/1 ContainerCreating 0 35s default example-6c5cdc6fb6-zj2zk 0/1 ContainerCreating 0 35s default example-6c5cdc6fb6-k2fkm 0/1 ContainerCreating 0 35s Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle IP, but ReservedIP will still ensure that no Pod can continue to use this IP address when this Pod releases its IP. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work. A Trap So, can we use IPPool's field excludeIPs to achieve the same effect as ReservedIP CRD? The answer is NO ! Look at such a case, now we want to reserve an IP 172.18.60.31 for an external application of the cluster, which may be a Redis node. To achieve this, we created such an IPPool: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IPv4-IP-pool-already-in-use spec: ipVersion: 4 subnet: 172.18.60.0/24 ips: - 172.18.60.1-172.18.60.31 excludeIPs: - 172.18.60.31 I believe that if there is only one IPPool under the subnet 172.18.60.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool (Different IPPools allow to define the same subnet, more details of validating of IPPool CRD ): apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IPv4-IP-pool-created-by-someone spec: ipVersion: 4 subnet: 172.18.60.0/24 ips: - 172.18.60.31-172.18.60.50 After a period of time, a Pod may be allocated IP 172.18.60.31 from IPPool IPv4-IP-pool-created-by-someone , and then it holds the same IP address as your Redis node. After that, your Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, ReservedIP CRD makes life better :)","title":"Reserved IP"},{"location":"usage/reserved-ip/#reserved-ip","text":"Spiderpool reserve some IP addresses for the whole cluster, which will not be used by any IPAM allocation. Usually, these IP addresses are some external IP addresses.","title":"Reserved IP"},{"location":"usage/reserved-ip/#ippool-excludeips","text":"First of all, you may have observed that there is an excludeIPs field in IPPool CRD. To some extent, it is also a mechanism for reserving IP addresses, but its main work is not so. Honestly, excludeIPs field is more of a syntax sugar , so that users can define their IPPool CRD more flexibly. For example, now we want to create an IPPool, which contains two IP ranges: 172.18.40.40-172.18.40.44 and 172.18.40.46-172.18.40.50 . Without using excludeIPs , we may need the following IPPool manifest: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: not-use-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.44 - 172.18.40.46-172.18.40.50 But in fact, we can more concisely describe this semantics through excludeIPs : apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: use-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.50 excludeIPs: - 172.18.40.45 Of course, ExcludeIPs also supports the format of IP range and we can define multiple excludeIPs records to segment a subnet in more detail: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IP-ranges-in-excludeIPs spec: ipVersion: 4 subnet: 172.18.40.0/24 ips: - 172.18.40.40-172.18.40.50 excludeIPs: - 172.18.40.45 - 172.18.40.47-172.18.40.49 excludeIPs will make sure that any Pod that allocates IP from this IPPool will not use these excluded IP addresses. However, it should be noted that this mechanism only has an effect on the IPPool itself with excludeIPs defined.","title":"IPPool excludeIPs"},{"location":"usage/reserved-ip/#reservedip","text":"Unlike excluedIPs field in IPPool CRD, ReservedIP CRD is actually used to define the global reserved IP rules of a cluster. ReservedIP ensures that no Pod in a cluster will use these IP addresses defined by it, whether or not some IPPools have inadvertently defined the same IP addresses for Pods use. For example, we have such an IPPool with 10 IP addresses from 172.18.50.40 to 172.18.50.50 : apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: test-IPv4-IP-pool spec: ipVersion: 4 subnet: 172.18.50.0/24 ips: - 172.18.50.40-172.18.50.50 Unfortunately, a ReservedIP has been pre created in cluster, which reserves 100 IP addresses from 172.18.50.1 to 172.18.50.100 : apiVersion: spiderpool.spidernet.io/v1 kind: ReservedIP metadata: name: reserved spec: ipVersion: 4 ips: - 172.18.50.1-172.18.50.100 Now, if we create a Deployment and let its Pods allocate IP addresses from IPPool test-IPv4-IP-pool : apiVersion: apps/v1 kind: Deployment metadata: name: example spec: replicas: 3 selector: matchLabels: app: example template: metadata: annotations: ipam.spidernet.io/ippool: |- { \"interface\": \"eth0\", \"ipv4pools\": [\"test-IPv4-IP-pool\"], } labels: app: example spec: containers: - image: alpine imagePullPolicy: IfNotPresent name: example command: [\"/bin/sh\", \"-c\", \"trap : TERM INT; sleep infinity & wait\"] None of these Pods can run successfully because \"all IP addresses are used out\". But when we remove this ReservedIP reserved , everything will return to normal. $ kubectl get po -l app=example NAMESPACE NAME READY STATUS RESTARTS AGE default example-6c5cdc6fb6-hvzrp 0/1 ContainerCreating 0 35s default example-6c5cdc6fb6-zj2zk 0/1 ContainerCreating 0 35s default example-6c5cdc6fb6-k2fkm 0/1 ContainerCreating 0 35s Another interesting question is that what happens if an IP address to be reserved has been allocated before ReservedIP is created? Of course, we dare not stop this running Pod and recycle IP, but ReservedIP will still ensure that no Pod can continue to use this IP address when this Pod releases its IP. Therefore, ReservedIPs should be confirmed as early as possible before network planning, rather than being supplemented at the end of all work.","title":"ReservedIP"},{"location":"usage/reserved-ip/#a-trap","text":"So, can we use IPPool's field excludeIPs to achieve the same effect as ReservedIP CRD? The answer is NO ! Look at such a case, now we want to reserve an IP 172.18.60.31 for an external application of the cluster, which may be a Redis node. To achieve this, we created such an IPPool: apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IPv4-IP-pool-already-in-use spec: ipVersion: 4 subnet: 172.18.60.0/24 ips: - 172.18.60.1-172.18.60.31 excludeIPs: - 172.18.60.31 I believe that if there is only one IPPool under the subnet 172.18.60.0/24 network segment in cluster, there will be no problem and it can even work perfectly. Unfortunately, your friends may not know about it, and then he/she created such an IPPool (Different IPPools allow to define the same subnet, more details of validating of IPPool CRD ): apiVersion: spiderpool.spidernet.io/v1 kind: IPPool metadata: name: IPv4-IP-pool-created-by-someone spec: ipVersion: 4 subnet: 172.18.60.0/24 ips: - 172.18.60.31-172.18.60.50 After a period of time, a Pod may be allocated IP 172.18.60.31 from IPPool IPv4-IP-pool-created-by-someone , and then it holds the same IP address as your Redis node. After that, your Redis may not work as well. So, if you really want to reserve an IP address instead of excluding an IP address, ReservedIP CRD makes life better :)","title":"A Trap"},{"location":"usage/statefulset/","text":"StatefulSet Description The spiderpool supports IP assignment for StatefulSet. when the replica number of statefulset is not scaled up or down, all pods could hold same IP address even pod restarting or rebuilding happens. Pod restarts Once a pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod deleted and re-creates After deleting a StatefulSet pod, kubernetes will re-create a pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID. Notice Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its pods are running. When the statefulset is scaled down and then scaled up, the scaled-up pod is not guaranteed to get the IP of scaled-down pod event they have the same name The RIPOGT feature ( reclaim IP for the pod of graceful-period timeout ) does work for statefulset pod. Get Started Enable StatefulSet support Firstly, please ensure you have installed the spiderpool and configure the CNI file, refer install for details Check configmap spiderpool-conf property enableStatefulSet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade --set configmap.config.enableStatefulSet=true Create a StatefulSet install a StatefulSet example kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset-demo.md Validate the Spiderpool related CR data Here's the created Pod, spiderippool, spiderendpoint CR information: ```text $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 82s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ... ``` Try to delete pod demo-sts-0 and check whether the rebuilding pod keeps the previous IP or not. ```text $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 20s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker history: - containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker - containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ownerControllerType: StatefulSet ... ``` And you can see, the re-create Pod still hold the previous IP, and spiderippool, spiderendpoint updated containerID property. clean up delete StatefulSet object demo-sts . ```text $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found ``` The related data was cleaned up.","title":"StatefulSet"},{"location":"usage/statefulset/#statefulset","text":"","title":"StatefulSet"},{"location":"usage/statefulset/#description","text":"The spiderpool supports IP assignment for StatefulSet. when the replica number of statefulset is not scaled up or down, all pods could hold same IP address even pod restarting or rebuilding happens. Pod restarts Once a pod restarts (its pause container restarts), Spiderpool will keep use the previous IP, and change the IPPool CR property ContainerID with the new pause container ID. In the meanwhile, spiderendpoint will still keep the previous IP but refresh the ContainerID property. Pod deleted and re-creates After deleting a StatefulSet pod, kubernetes will re-create a pod with the same name. In this case, Spiderpool will also keep the previous IP and update the ContainerID.","title":"Description"},{"location":"usage/statefulset/#notice","text":"Currently, it's not allowed to change StatefulSet annotation for using another pool when a StatefulSet is ready and its pods are running. When the statefulset is scaled down and then scaled up, the scaled-up pod is not guaranteed to get the IP of scaled-down pod event they have the same name The RIPOGT feature ( reclaim IP for the pod of graceful-period timeout ) does work for statefulset pod.","title":"Notice"},{"location":"usage/statefulset/#get-started","text":"","title":"Get Started"},{"location":"usage/statefulset/#enable-statefulset-support","text":"Firstly, please ensure you have installed the spiderpool and configure the CNI file, refer install for details Check configmap spiderpool-conf property enableStatefulSet whether is already set to true or not. kubectl -n kube-system get configmap spiderpool-conf -o yaml If you want to set it true , just execute helm upgrade --set configmap.config.enableStatefulSet=true","title":"Enable StatefulSet support"},{"location":"usage/statefulset/#create-a-statefulset","text":"install a StatefulSet example kubectl apply -f https://raw.githubusercontent.com/spidernet-io/spiderpool/main/docs/example/statefulset-demo.md","title":"Create a StatefulSet"},{"location":"usage/statefulset/#validate-the-spiderpool-related-cr-data","text":"Here's the created Pod, spiderippool, spiderendpoint CR information: ```text $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 82s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ... ``` Try to delete pod demo-sts-0 and check whether the rebuilding pod keeps the previous IP or not. ```text $ kubectl delete po demo-sts-0 pod \"demo-sts-0\" deleted $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demo-sts-0 1/1 Running 0 20s 172.18.40.47 spider-worker $ kubectl get sp default-v4-ippool -o yaml ... 172.18.40.47: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 interface: eth0 namespace: default node: spider-worker ownerControllerType: StatefulSet pod: demo-sts-0 ... $ kubectl get se demo-sts-0 -o yaml ... status: current: containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker history: - containerID: 5c7a1c9cf494c02090848bd3f8131817d02ee9f3046cd33a5ec4b74b897d6789 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker - containerID: cffbddab79dc8eea447315da9b84db402d515b657d2b6943a87b47cdfa876359 creationTime: \"2022-07-29T08:54:12Z\" ips: - interface: eth0 ipv4: 172.18.40.47/16 ipv4Gateway: \"\" ipv4Pool: default-v4-ippool ipv6: fc00:f853:ccd:e793:f::63/64 ipv6Gateway: \"\" ipv6Pool: default-v6-ippool vlan: 0 node: spider-worker ownerControllerType: StatefulSet ... ``` And you can see, the re-create Pod still hold the previous IP, and spiderippool, spiderendpoint updated containerID property.","title":"Validate the Spiderpool related CR data"},{"location":"usage/statefulset/#clean-up","text":"delete StatefulSet object demo-sts . ```text $ kubectl delete sts demo-sts statefulset.apps \"demo-sts\" deleted $ kubectl get sp default-v4-ippool -o yaml | grep demo-sts-0 $ kubectl get se demo-sts-0 -o yaml Error from server (NotFound): spiderendpoints.spiderpool.spidernet.io \"demo-sts-0\" not found ``` The related data was cleaned up.","title":"clean up"}]}